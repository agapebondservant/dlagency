{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6330dd4-07ca-4d37-90da-ef6349e23061",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ADD8E6; border: 1px solid gray; padding: 3px\">\n",
    "        <h3>Data Generation Workflow</h3>\n",
    "        <li><b>Data Extraction</b>: Extracts data from markdown files using LangChain.</li>\n",
    "        <li><b>Data Generation</b>: Generates data records using sdg_hub.</li>\n",
    "        <li><b>Data Uploads</b>: Uploads generated data to MinIO.</li>\n",
    "        <li><b>Data Population</b>: Builds LanceDB store.</li>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b93a3d50-53e8-4dfa-a2ad-78bad0013189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###################################################################################################\n",
    "# Imports\n",
    "###################################################################################################\n",
    "import boto3\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "from langchain_community.vectorstores import LanceDB\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.graph_vectorstores import GraphVectorStoreRetriever\n",
    "from langchain_core.documents import Document\n",
    "from lancedb.rerankers import LinearCombinationReranker\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "import lancedb\n",
    "from huggingface_hub import snapshot_download\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings, SentenceTransformerEmbeddings\n",
    "from transformers import AutoTokenizer\n",
    "from enum import Enum\n",
    "import traceback\n",
    "import numpy as np\n",
    "import re\n",
    "import uuid\n",
    "import pandas as pd\n",
    "import pprint\n",
    "from datasets import Dataset\n",
    "from sdg_hub.core.flow import FlowRegistry, Flow\n",
    "from flow_extensions import CustomDeleteColumnsBlock\n",
    "import nest_asyncio\n",
    "import litellm\n",
    "import json\n",
    "from tabulate import tabulate\n",
    "nest_asyncio.apply()\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62b2b8a8-8c46-4a3a-b26b-254ca4c6984b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11385/2306557545.py:10: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "###################################################################################################\n",
    "# Instance variables\n",
    "###################################################################################################\n",
    "endpoint_url = os.getenv('AWS_S3_ENDPOINT')\n",
    "access_key_id = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "bucket = os.getenv(\"AWS_S3_BUCKET\")\n",
    "target_path = \"data\"\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\", \n",
    "    model_kwargs={\"trust_remote_code\":True}\n",
    ")\n",
    "\n",
    "db = lancedb.connect(f\"s3://data/lancedb-dla\",\n",
    "    storage_options={\n",
    "        \"endpoint_url\": endpoint_url,\n",
    "        \"aws_access_key_id\": access_key_id,\n",
    "        \"aws_secret_access_key\": secret_access_key,\n",
    "        \"s3_force_path_style\": \"true\",\n",
    "        \"allow_http\": \"true\",\n",
    "    }\n",
    ")\n",
    "\n",
    "minio = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url=endpoint_url,\n",
    "    aws_access_key_id=access_key_id,\n",
    "    aws_secret_access_key=secret_access_key,\n",
    "    config=boto3.session.Config(signature_version='s3v4')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d447e203-614b-4f0f-9551-a5216e4d893c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################\n",
    "# Generate raw data\n",
    "###################################################################################################\n",
    "\n",
    "def build_raw_dataset(target_dir, suffix=\".md\", prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Extracts STIG data from markdown files.\n",
    "    \"\"\"\n",
    "    try:\n",
    "\n",
    "        data, current_group_content = [], None\n",
    "\n",
    "        files = os.listdir(target_dir)\n",
    "\n",
    "        files = [f for f in files if f.endswith(\".md\") and f.startswith(prefix)]\n",
    "\n",
    "        for file in files:\n",
    "            \n",
    "            filecontent = None\n",
    "            \n",
    "            with open(f\"{target_dir}/{file}\", mode=\"r\") as f: \n",
    "                \n",
    "                filecontent = f.read()\n",
    "                \n",
    "            headers_to_split = [(\"##\", \"Group\"), (\"###\", \"Rule\")]\n",
    "            \n",
    "            text_splitter = MarkdownHeaderTextSplitter(headers_to_split, strip_headers=False)\n",
    "            \n",
    "            splits = text_splitter.split_text(filecontent)\n",
    "    \n",
    "            for split in splits:\n",
    "    \n",
    "                if \"Group\" in split.metadata and \"Rule\" not in split.metadata:\n",
    "    \n",
    "                    current_group_content = split.page_content\n",
    "                \n",
    "                if \"Group\" in split.metadata and \"Rule\" in split.metadata:\n",
    "                \n",
    "                    content = f\"\"\"{current_group_content.strip()}{split.page_content.strip()}\"\"\"\n",
    "                    \n",
    "                    data.append({\"id\": str(uuid.uuid4()),\n",
    "                                \"text\": content,\n",
    "                                })   \n",
    "\n",
    "        # pprint.pprint(data)\n",
    "    \n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        ds = Dataset.from_pandas(df)\n",
    "        \n",
    "        return ds\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting data from {target_dir}: {e}\")\n",
    "        \n",
    "        traceback.print_exc() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7c31e2d-2e4d-4bcb-a5b8-d00c83a2346d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################\n",
    "# Generate synthetic data\n",
    "###################################################################################################\n",
    "def build_synthetic_dataset(ds):\n",
    "    from datasets import load_dataset, DatasetDict\n",
    "    \n",
    "    flow_path = \"flows/rag_knowledge_generation/flow.yaml\"\n",
    "    \n",
    "    flow = Flow.from_yaml(flow_path)\n",
    "    \n",
    "    flow.set_model_config(\n",
    "        model=os.getenv(\"LLM_ID\"),\n",
    "        \n",
    "        api_key=os.getenv('LLM_TOKEN'),\n",
    "        \n",
    "        api_base=os.getenv('LLM_API_BASE'),\n",
    "        \n",
    "        temperature=0.1,\n",
    "    )\n",
    "        \n",
    "    dataset = flow.generate(ds, max_concurrency=100)\n",
    "    \n",
    "    dataset.to_json(f\"{target_path}/stig_data.jsonl\")\n",
    "\n",
    "    dataset.to_parquet(f\"{target_path}/stig_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1473f11a-5e0d-4462-bb24-43e0de745872",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################\n",
    "# Upload data to LanceDB\n",
    "###################################################################################################\n",
    "# Migrate Local Search tables\n",
    "\n",
    "def upload_to_lancedb(parquet_dir, connection):\n",
    "    \"\"\"\n",
    "    Uploads the parquet file(s) in the provided directory to the specified lancedb database connection.\n",
    "    \"\"\"\n",
    "    print(\"Migrating data to LanceDB...\")\n",
    "    \n",
    "    try:\n",
    "        for file_path in os.listdir(f\"{target_path}\"):\n",
    "            \n",
    "            if file_path.endswith(\".parquet\"):\n",
    "        \n",
    "                try:\n",
    "            \n",
    "                    full_path = os.path.join(target_path, file_path)\n",
    "        \n",
    "                    df = pd.read_parquet(full_path)\n",
    "\n",
    "                    text_embeddings = embedding_model.embed_documents(df['text'].tolist())\n",
    "\n",
    "                    log_embeddings = embedding_model.embed_documents(df['log_entry'].tolist())\n",
    "                    \n",
    "                    df['vector'] = [np.array(e, dtype=np.float32).tolist() for e in text_embeddings]\n",
    "\n",
    "                    df['logs_vector'] = [np.array(e, dtype=np.float32).tolist() for e in log_embeddings]\n",
    "        \n",
    "                    table_name = file_path.split(\".\", 1)[0]\n",
    "        \n",
    "                    table = connection.create_table(table_name, data=df)\n",
    "\n",
    "                    table.create_fts_index(\"text\")\n",
    "\n",
    "                    table.create_fts_index(\"log_entry\")\n",
    "\n",
    "                    table.create_index(metric=\"cosine\", vector_column_name=\"vector\")\n",
    "\n",
    "                    table.create_index(metric=\"cosine\", vector_column_name=\"logs_vector\")\n",
    "        \n",
    "                    print(f\"{table_name} migrated.\")\n",
    "        \n",
    "                except Exception as e:\n",
    "                    \n",
    "                    print(f\"Error uploading data from {full_path}: {e}\")    \n",
    "            \n",
    "        print(\"Migration complete.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        \n",
    "        print(f\"Error uploading data to LanceDB: {e}\")\n",
    "\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ad7412-a1fe-465d-b919-77d8042a9b58",
   "metadata": {},
   "source": [
    "## Run data generation pipeline\n",
    "Run the pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99ad7234-279e-499d-92c4-a4818ca6c1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Migrating data to LanceDB...\n",
      "stig_data migrated.\n",
      "Migration complete.\n",
      "Transformation complete.\n"
     ]
    }
   ],
   "source": [
    "# os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# ds = build_raw_dataset(\"markdown\", prefix=\"U_RH\")\n",
    "\n",
    "# build_synthetic_dataset(ds)\n",
    "\n",
    "upload_to_lancedb(target_path, db)\n",
    "\n",
    "print(f\"Transformation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b09c1acf-5a75-466e-9a99-95b010fb1841",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################\n",
    "# Test LanceDB Query\n",
    "###################################################################################################\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"GRANITE_LLM_TOKEN\"),\n",
    "    \n",
    "    base_url=os.getenv('GRANITE_LLM_BASE_URL'),\n",
    ")\n",
    "\n",
    "def retrieve_documents(logs: str):\n",
    "    table = db.open_table(\"stig_data\")\n",
    "    \n",
    "    # query_vector = embedding_model.embed_query(logs).astype(np.float32).tolist()\n",
    "\n",
    "    query_vector = np.array(embedding_model.embed_query(logs), dtype=np.float32).tolist()\n",
    "\n",
    "    reranker = LinearCombinationReranker() \n",
    "    \n",
    "    results = table.search(query_type=\"hybrid\",vector_column_name=\"vector\").vector(query_vector).text(logs).rerank(reranker=reranker).select([\"text\", \"log_entry\"]).limit(3).to_list()\n",
    "\n",
    "    return results\n",
    "    \n",
    "\n",
    "def run_query(body: dict) -> dict:\n",
    "    prompt = body.get(\"prompt\")\n",
    "    \n",
    "    max_tokens = body.get(\"max_tokens\", 8192)\n",
    "    \n",
    "    temperature = body.get(\"temperature\", 0.1)\n",
    "    \n",
    "    top_p = body.get(\"top_p\", 1)\n",
    "    \n",
    "    n = body.get(\"n\", 1)\n",
    "    \n",
    "    retrieved_docs = retrieve_documents(prompt)\n",
    "\n",
    "    # print(json.dumps(retrieved_docs, indent=4))\n",
    "    \n",
    "    context = \"\\n\\n\".join([f\"### Sample Logs (Relevance Score={doc['_relevance_score']}):\\n{doc['log_entry']}\\n\\n### Violated STIG control:\\n{doc['text']}\" for doc in retrieved_docs])\n",
    "    \n",
    "    print(context)\n",
    "\n",
    "    # 2. Construct the prompt with context\n",
    "    user_message = f\"\"\"\n",
    "    You are a helpful assistant with expertise in system engineering and cybersecurity. \n",
    "    Provide an analysis of the following RHEL logs and system configuration against the relevant STIG compliance requirements. \n",
    "    Identify specific violations and recommend a prioritized course of action to remediate each non-compliant finding, \n",
    "    including estimated effort and impact.\n",
    "    Also identify the priority level of the violation based on the STIG category.\n",
    "    Where appropriate, use the context provided here:\n",
    "    {context}\n",
    "    \n",
    "    Logs: \n",
    "    {prompt}\n",
    "    \n",
    "    Analysis:\n",
    "    \"\"\"\n",
    "\n",
    "    # 3. Send the request to the local LLM using the OpenAI client\n",
    "    try:\n",
    "        response = litellm.completion(\n",
    "            model=f\"hosted_vllm/{os.getenv('GRANITE_LLM_ID')}\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": user_message}\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            top_p=top_p,\n",
    "            n=n,\n",
    "            api_key=os.getenv(\"GRANITE_LLM_TOKEN\"),\n",
    "            base_url=os.getenv('GRANITE_LLM_BASE_URL'),\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error communicating with local LLM: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3bc5901f-8a42-4b7a-825b-0805f73d6396",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################\n",
    "# Sample Logs\n",
    "###################################################################################################\n",
    "prompt1 = \"\"\"\n",
    "× httpd.service - The Apache HTTP Server\n",
    "     Loaded: loaded (/usr/lib/systemd/system/httpd.service; disabled; preset: disabled)\n",
    "     Active: failed (Result: exit-code) since Tue 2025-04-08 15:56:15 UTC; 1h 11min ago\n",
    "   Duration: 1min 9.506s\n",
    "       Docs: man:httpd.service(8)\n",
    "    Process: 4099 ExecStart=/usr/sbin/httpd $OPTIONS -DFOREGROUND (code=exited, status=1/FAILURE)\n",
    "   Main PID: 4099 (code=exited, status=1/FAILURE)\n",
    "     Status: \"Reading configuration...\"\n",
    "        CPU: 58ms\n",
    "\n",
    "Apr 08 15:56:15 node1.example.com systemd[1]: Starting The Apache HTTP Server...\n",
    "Apr 08 15:56:15 node1.example.com httpd[4099]: AH00526: Syntax error on line 35 of /etc/httpd/conf/httpd.conf:\n",
    "Apr 08 15:56:15 node1.example.com httpd[4099]: Invalid command 'InvalidDirectiveHere', perhaps misspelled or defined by a module not included in the server configuration\n",
    "Apr 08 15:56:15 node1.example.com systemd[1]: httpd.service: Main process exited, code=exited, status=1/FAILURE\n",
    "Apr 08 15:56:15 node1.example.com systemd[1]: httpd.service: Failed with result 'exit-code'.\n",
    "Apr 08 15:56:15 node1.example.com systemd[1]: Failed to start The Apache HTTP Server.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2e2ea870-5e7f-452b-8899-bc4f76469df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[90m[\u001b[0m2025-12-02T12:31:45Z \u001b[33mWARN \u001b[0m lance::dataset::scanner\u001b[90m]\u001b[0m Deprecation warning, this behavior will change in the future. This search specified output columns but did not include `_distance`.  Currently the `_distance` column will be included.  In the future it will not.  Call `disable_scoring_autoprojection` to to adopt the future behavior and avoid this warning\n",
      "\u001b[90m[\u001b[0m2025-12-02T12:31:45Z \u001b[33mWARN \u001b[0m lance::dataset::scanner\u001b[90m]\u001b[0m Deprecation warning, this behavior will change in the future. This search specified output columns but did not include `_score`.  Currently the `_score` column will be included.  In the future it will not.  Call `disable_scoring_autoprojection` to adopt the future behavior and avoid this warning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Sample Logs (Relevance Score=1.0):\n",
      "```\n",
      "$ sudo systemctl status ctrl-alt-del.target\n",
      "● ctrl-alt-del.target - Ctrl-Alt-Delete Target\n",
      "   Loaded: loaded (/usr/lib/systemd/system/ctrl-alt-del.target; enabled)\n",
      "   Active: active (running) since Mon 2025-12-01 10:00:00 UTC; 1h 23min ago\n",
      "   Docs: man:systemd.special(7)\n",
      "```\n",
      "\n",
      "### Violated STIG control:\n",
      "## Group: SRG-OS-000324-GPOS-00125  \n",
      "**Group ID:** `V-257785`### Rule: The x86 Ctrl-Alt-Delete key sequence must be disabled on RHEL 9.  \n",
      "**Rule ID:** `SV-257785r925342_rule`\n",
      "**Severity:** high  \n",
      "**Description:**\n",
      "<VulnDiscussion>A locally logged-on user who presses Ctrl-Alt-Delete when at the console can reboot the system. If accidentally pressed, as could happen in the case of a mixed OS environment, this can create the risk of short-term loss of availability of systems due to unintentional reboot. In a graphical user environment, risk of unintentional reboot from the Ctrl-Alt-Delete sequence is reduced because the user will be prompted before any action is taken. Satisfies: SRG-OS-000324-GPOS-00125, SRG-OS-000480-GPOS-00227</VulnDiscussion><FalsePositives></FalsePositives><FalseNegatives></FalseNegatives><Documentable>false</Documentable><Mitigations></Mitigations><SeverityOverrideGuidance></SeverityOverrideGuidance><PotentialImpacts></PotentialImpacts><ThirdPartyTools></ThirdPartyTools><MitigationControl></MitigationControl><Responsibility></Responsibility><IAControls></IAControls>  \n",
      "**Check Text:**\n",
      "Verify RHEL 9 is not configured to reboot the system when Ctrl-Alt-Delete is pressed with the following command: $ sudo systemctl status ctrl-alt-del.target ctrl-alt-del.target Loaded: masked (Reason: Unit ctrl-alt-del.target is masked.) Active: inactive (dead) If the \"ctrl-alt-del.target\" is loaded and not masked, this is a finding.\n",
      "\n",
      "### Sample Logs (Relevance Score=0.7547379732131958):\n",
      "**Log entry indicating the SSH service is not running (violation of V‑204586):**\n",
      "\n",
      "```\n",
      "Jan 10 14:32:07 server systemd[1]: Stopped OpenSSH server daemon.\n",
      "Jan 10 14:32:07 server systemd[1]: sshd.service: Failed with result 'exit-code'.\n",
      "Jan 10 14:32:07 server systemd[1]: sshd.service: Failed to start OpenSSH server daemon.\n",
      "```\n",
      "\n",
      "**Result of `systemctl status sshd.service`:**\n",
      "\n",
      "```\n",
      "● sshd.service - OpenSSH server daemon\n",
      "   Loaded: loaded (/usr/lib/systemd/system/sshd.service; disabled)\n",
      "   Active: inactive (dead) since Mon 2025-01-10 14:32:07 EST; 2s ago\n",
      "  Process: 1234 ExecStart=/usr/sbin/sshd -D (code=exited, status=1/FAILURE)\n",
      "```\n",
      "\n",
      "### Violated STIG control:\n",
      "## Group: SRG-OS-000423-GPOS-00187  \n",
      "**Group ID:** `V-204586`### Rule: The Red Hat Enterprise Linux operating system must be configured so that all networked systems use SSH for confidentiality and integrity of transmitted and received information as well as information during preparation for transmission.  \n",
      "**Rule ID:** `SV-204586r958908_rule`\n",
      "**Severity:** medium  \n",
      "**Description:**\n",
      "<VulnDiscussion>Without protection of the transmitted information, confidentiality and integrity may be compromised because unprotected communications can be intercepted and either read or altered. This requirement applies to both internal and external networks and all types of information system components from which information can be transmitted (e.g., servers, mobile devices, notebook computers, printers, copiers, scanners, and facsimile machines). Communication paths outside the physical protection of a controlled boundary are exposed to the possibility of interception and modification. Protecting the confidentiality and integrity of organizational information can be accomplished by physical means (e.g., employing physical distribution systems) or by logical means (e.g., employing cryptographic techniques). If physical means of protection are employed, then logical means (cryptography) do not have to be employed, and vice versa. Satisfies: SRG-OS-000423-GPOS-00187, SRG-OS-000424-GPOS-00188, SRG-OS-000425-GPOS-00189, SRG-OS-000426-GPOS-00190</VulnDiscussion><FalsePositives></FalsePositives><FalseNegatives></FalseNegatives><Documentable>false</Documentable><Mitigations></Mitigations><SeverityOverrideGuidance></SeverityOverrideGuidance><PotentialImpacts></PotentialImpacts><ThirdPartyTools></ThirdPartyTools><MitigationControl></MitigationControl><Responsibility></Responsibility><IAControls></IAControls>  \n",
      "**Check Text:**\n",
      "Verify SSH is loaded and active with the following command: # systemctl status sshd sshd.service - OpenSSH server daemon Loaded: loaded (/usr/lib/systemd/system/sshd.service; enabled) Active: active (running) since Tue 2015-11-17 15:17:22 EST; 4 weeks 0 days ago Main PID: 1348 (sshd) CGroup: /system.slice/sshd.service 1053 /usr/sbin/sshd -D If \"sshd\" does not show a status of \"active\" and \"running\", this is a finding.\n",
      "\n",
      "### Sample Logs (Relevance Score=0.699999988079071):\n",
      "Jan 01 12:00:00 hostname systemd[1]: Starting Journal Service...\n",
      "Jan 01 12:00:00 hostname systemd[1]: Failed to start Journal Service: Unit systemd-journald.service not found.\n",
      "\n",
      "### Violated STIG control:\n",
      "## Group: SRG-OS-000269-GPOS-00103  \n",
      "**Group ID:** `V-257783`### Rule: RHEL 9 systemd-journald service must be enabled.  \n",
      "**Rule ID:** `SV-257783r925336_rule`\n",
      "**Severity:** medium  \n",
      "**Description:**\n",
      "<VulnDiscussion>In the event of a system failure, RHEL 9 must preserve any information necessary to determine cause of failure and any information necessary to return to operations with least disruption to system processes.</VulnDiscussion><FalsePositives></FalsePositives><FalseNegatives></FalseNegatives><Documentable>false</Documentable><Mitigations></Mitigations><SeverityOverrideGuidance></SeverityOverrideGuidance><PotentialImpacts></PotentialImpacts><ThirdPartyTools></ThirdPartyTools><MitigationControl></MitigationControl><Responsibility></Responsibility><IAControls></IAControls>  \n",
      "**Check Text:**\n",
      "Verify that \"systemd-journald\" is active with the following command: $ systemctl is-active systemd-journald active If the systemd-journald service is not active, this is a finding.\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Error communicating with local LLM: litellm.BadRequestError: Hosted_vllmException - Your authentication token is not from a valid issuer.'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "run_query({\n",
    "    \"prompt\": prompt1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60bb1358-4f4b-48a2-a7de-664f8226569b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "145b9094-84d8-476d-aad0-a78adc100616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325dd59e-61c9-44bd-aa81-1ba6bec56546",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b9e49e-f74b-4c7a-9ab9-75cc1eefdcff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
